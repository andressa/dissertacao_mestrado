\chapter{Mineração de códigos fonte para identificação de falhas}
\label{code_mining}
Em desenvolvimento de software é comum observar que quanto maior for o tempo de permanência de um erro no código mais custosa torna-se a sua correção e manutenção de um projeto~\cite{IEEE-Software-McConnel}. Falhas em códigos fonte são definidas como ``Imperfeições durante o desenvolvimento de software que como consequência impedem que o programa atinja as expectativas desejadas."~\cite{BASKARAN2010}. Entende-se ainda que muitos defeitos são introduzidos durante o processo de desenvolvimento como um todo, não somente no seu início ou fim. Assim sendo, identificar falhas antecipadamente torna-se parte essencial em desenvolvimento de software, que significa qualidade na entrega final do produto.~\cite{ODC1992}.

O SDLC (do inglês, \emph{software development life cycle} ou ciclo de vida do desenvolvimento de software)~\cite{6906040} indica que o desenvolvimento de software pode ser descrito em cinco etapas:
\begin{enumerate}
  \item{Levantamento de Requisitos}
  \item{Projeto}
  \item{Implementação}
  \item{Teste}
  \item{Implantação}
\end{enumerate}

A figura~\ref{fig:bug_insertion} ilustra em quais etapas do ciclo de vida de desenvolvimento do software são inseridos erros em projetos. Entidades presentes na área de engenharia de computação (tais como a IBM~\cite{IBM} e HCL~\cite{HCL}) confirmam que cerca de 35\% das falhas identificadas em códigos fonte são inseridas na fase de implementação~\cite{CODEMINING2015}.

\begin{figure}[h]
  \centering
  \includegraphics[width=8cm]{images/bug_insertion_sdlc.png}
  \caption{Estimativa do percentual de erros inseridos durante fases do SDLC. (Fonte: \emph{Computer Finance Magazine}. Dados extraídos de~\cite{CODEMINING2015})}
  \label{fig:bug_insertion}
\end{figure}

Atualmente, diferentes técnicas tem sido utilizadas para identificação antecipada de erros em códigos fonte. A análise estática, por exemplo, é bastante utilizada na área de programação desde a década de 80. As características desta análise são abordadas em~\ref{static_analysis}. Em conjunto com analisadores estáticos, pode-se aplicar técnicas de mineração de dados, fazendo uma combinação entre a área de engenharia de software e inteligência computacional. No artigo~\cite{4222731}, publicado em 2007, os autores abordam como a mineração de dados pode contribuir com a área de engenharia de software, na medida que técnicas de inteligência computacional, quando aplicadas, melhoram a qualidade de projetos. Nesta mesma direção, a MSR (em inglês, \emph{Mining Software Repositories} ou mineração de repositórios de códigos) passa a ser amplamente abordada devido a disponibilidade gratuita de inúmeros repositórios públicos de controle de versionamento, rastreamento de erros inseridos e até mesmo documentações~\cite{4659248}.

Aplicar mineração de dados em códigos fonte desperta interesse na comunidade científica, uma vez que códigos são tipicamente estruturados e semanticamente ricos em termos de construtores de programação (tais como variáveis, funções, objetos estruturados e rotinas bem definidas). Os objetivos são diversos: vão desde a tentativa de descobrir o que um determinado software faz (sem necessariamente executá-lo) até manutenção de projetos ou análises de desenvolvimento~\cite{130589}.

Como citado, a identificação antecipada de erros inseridos durante a implementação de códigos fonte é atraente do ponto de vista de qualidade de produto. A utilização de mineração de códigos é uma das aplicações mais ativas em engenharia de software atualmente. Neste caso, o objetivo é obter ferramentas que não só identifiquem falhas (ou \emph{bugs}) mas também sua localização exata nas linhas de códigos do projeto, de tal forma que o seu conserto seja facilitado~\cite{130589}. Para alcançar esse objetivo, uma direção de bastante destaque consiste na mineração de padrões, que uma vez pré-definidos, são aplicadas em diversos repositórios de projetos já existentes com o objetivo de encontrar anomalias comuns que violam regras (~\cite{Li05pr-miner:automatically},~\cite{4222586},~\cite{Chang:2007:FWN:1273463.1273486}).

Outro objetivo dominante na mineração de códigos é a tentativa de identificar fragmentos clonados. A não reutilização de códigos dificulta a manutenção do software e ainda, pode proliferar \emph{bugs} em diferentes partes de um projeto.

A seção~\ref{categories} aborda quais são os atributos comumente utilizados quando mineração de dados é aplicada a códigos fonte. Esta seção é composta por duas sub-seções (~\ref{static_analysis} e~\ref{halstead}) que abordam em que situações tais características foram utilizadas na aplicação de mineração de códigos neste mestrado; A seção~\ref{code_mining_cern} investiga a aplicação de mineração de dados em códigos fonte no CERN.

    \section{Seleção de atributos}
    \label{categories}
    Uma revisão feita por Heckman~\cite{Heckman:2011:SLR:1945085.1945215} com 21 artigos sobre mineração de dados em códigos fonte, identifica cinco categorias comumente utilizadas como entradas:
    \begin{enumerate}
      \item{\emph{Alert Characteristics} (AC): abordagem que leva em consideração os alertas gerados por analisadores estáticos (sub-seção~\ref{static_analysis});}
      \item{\emph{Code Characteristics} (CC): abordagem considera medidas estatísticas e de qualidade do código fonte atributos de entrada (sub-seção~\ref{halstead});}
      \item{\emph{Source code repository metrics} (SCR): atributos retirados de repositórios de códigos. Nesta abordagem, leva-se em consideração versionamentos (histórico de \emph{bugs} inseridos no projeto), comentários de atualizações e documentação sobre requisitos por exemplo;}
      \item{\emph{Bug database metrics} (BDM): Neste caso, existe uma base de dados para o projeto contendo todos as falhas já surgidas e consertadas. O objetivo dessa abordagem é identificar padrões que podem voltar a acontecer no projeto;}
      \item{\emph{Dynamic analyses metric} (DA): esta abordagem armazena resultados obtidos com análise dinâmica de códigos, mediante diversas execuções (ver seção~\ref{static_analysis})}.
    \end{enumerate}

    Tais categorias são descritas detalhadamente em~\cite{Heckman:2008}.

    Este mestrado utiliza as duas primeiras abordagens, descritas nas próximas sub-seções.

        \subsection{Analisadores Estáticos}
        \label{static_analysis}

        Em engenharia de software existem dois tipos de análise (complementares) que podem ser feitas para validar e testar o produto em concepção~\cite{1646907}:
        \begin{itemize}
          \item{\textbf{Estática}: neste caso apenas a estrutura do código é analisada, mas o código não é executado;
          }
          \item{\textbf{Dinâmica}: para esta análise, é preciso levantar um plano de testes, executá-los e avaliar os resultados. Ou seja, é necessário executar o código diversas vezes;
          }
        \end{itemize}

        Como o objetivo deste mestrado é analisar códigos fonte sem executá-los,  utilizar analisadores estáticos torna-se atraente, por definição.

        Na década de 80, diversas ferramentas que aplicam análise estática de código começaram a ser desenvolvidas para auxiliar desenvolvedores na implementação de códigos com menos \emph{bugs}~\cite{Tischler:1983:SAP:1006140.1006182}. Mas logo um inconveniente surge~\cite{YukselS13}: a quantidade de alertas gerados pelas ferramentas de análise estática de código (SCAT, do inglês, \emph{Static Code Analysis Tools}) é grande e muitos deles não são acionáveis. Diz-se acionável um alerta que realmente vai impedir o sucesso na execução de um determinado código. Essa classificação é feita manualmente pelo desenvolvedor (que neste caso é o especialista): se o especialista entende que determinado alerta é importante e precisa ser consertado antes da execução do programa, tal alerta é definido como \emph{acionável}. Caso contrário, ele é definido como \emph{não acionável}(~\cite{4815348},~\cite{Heckman:2008:EBE:1414004.1414013}).

        Empiricamente, existem cerca de 40 alertas para cada mil linhas de código (KLOC, do inglês, \emph{Kilo Lines of Code})~\cite{Heckman:2008:EBE:1414004.1414013}. Desta estimativa, podem ser não acionáveis 30-100\%, como observado em~\cite{Aggarwal:2006:ISD:1169228.1170032},~\cite{4026864},~\cite{Heckman:2008:EBE:1414004.1414013},~\cite{4228664},~\cite{Kim:2007:WIF:1287624.1287633},~\cite{Kremenek:2003:ZUS:1760267.1760289}. Tais números desestimulam desenvolvedores e projetistas a aplicar ferramentas de análise estática de código no processo de desenvolvimento de software, embora entende-se a sua importância no quesito qualidade adquirida. Por outro lado, um alerta definido pelo especialista como \emph{acionável} nem sempre vai impedir a execução bem sucedida do código. Ou seja, se por um lado os analisadores estáticos geram muitos alertas (\emph{não acionáveis}), nada garante que o alerta \emph{acionável} indica que o programa contém \emph{bugs} e vai falhar durante a tentativa de execução. Em outras palavras, um analisador estático pode não gerar nenhum alerta e mesmo assim, durante a tentativa de execução, o código ainda pode falhar.

        Para este mestrado, analisadores estáticos serão utilizados em uma segunda etapa, já que a classificação dos alertas gerados não significa exatamente que o código não pode ser executado. Uma vez que um código for classificado como \emph{não executável}, o objetivo seguinte será identificar quais alertas gerados por um analisador estático são acionáveis, para guiar o desenvolvedor no seu conserto, antes da execução de tal código. 


        \subsection{Medidas estatísticas e de qualidade}
        \label{halstead}

        Em computação, ``Halstead Metrics''~\cite{Halstead:1977:ESS:540137} foram medidas de qualidade e estatísticas definidas por Maurice Halstead e publicadas em 1977.

        As medidas estatísticas (de Halstead) que podem ser extraídas de um código fonte são:
        \begin{itemize}
          \item{$\eta_1$: quantidade de operadores distintos no código;}
          \item{$\eta_2$: quantidade de operandos distintos no código;}
          \item{$N_1$: total de operadores;}
          \item{$N_2$: total de operandos;}
        \end{itemize}

        Destas, outras medidas estatísticas são também definidas por Halstead:
        \begin{itemize}
          \item{Vocabulário: $\eta = \eta_1 + \eta_2$;}
          \item{Tamanho: $N = N_1 + N_2$;}
          \item{Volume: $V = N log_2 \eta$;}
          \item{Dificuldade: $D = \frac{\eta_1}{2} . \frac{N_2}{\eta_2}$;}
          \item{Esforço: $E = D . V$;}
          \item{Estimativa de tempo de execução (em segundos): $ T = \frac{E}{18}$;}
          \item{Estimativa de potenciais \emph{bugs}: $ B = \frac{V}{3000} $};
        \end{itemize}

        Outra característica que pode ser retirada de códigos fonte é a sua complexidade. Define-se como ``Cyclomatic Complexity'' (do inglês, complexidade ciclomática)~\cite{maccabe1983structured} o número de decisões que um determinado bloco de código contém mais uma unidade. Este número (também conhecido como \emph{McCabe number}) representa o número de caminhos independentes percorridos durante a execução de um código, quando se monta uma árvore abstrata de sintaxe (AST, do inglês, \emph{abstract syntax tree})~\cite{jones03pattern}.

        O índice de manutenabilidade (ou MI, do inglês \emph{Maintainability Index}) é uma medida em software que indica o quão fácil é a manutenção de um determinado código. O MI é calculado de diferentes maneiras, dependendo da abordagem que se deseja utilizar. Entretanto, a fórmula clássica~\cite{242525} envolve o número de linhas totais em um código fonte (SLOC, do inglês, \emph{source lines of code}), a complexidade ciclomática (neste contexto, CC) e o volume de Halstead (HV): \\ $MI = 171 - 5.2 ln _{HV} - 0.23 _{CC} - 16.2 ln _{SLOC}$.\\Os autores chegaram nesta fórmula através de um número de códigos fornecidos pela HP, escritos em linguagens de programação C e Pascal. Para cada código, um especialista (engenheiro de software) atribuiu uma nota (de 1 a 100) indicando o quão fácil seria fazer alterações ou correções posteriores ao determinado código (quanto maior a nota, mais fácil é sua manutenção). Consequentemente, 40 faixas de valores foram identificadas e através de uma regressão estatística, eventualmente, a fórmula exposta foi encontrada como um índice de manutenabilidade.

        As medidas descritas nesta sub-seção serão utilizadas neste mestrado, mediante análise de quais seriam mais importantes para o problema que se deseja resolver.

    \section{Analisadores estáticos e Mineração de códigos no CERN}
    \label{code_mining_cern}
    O CERN oferece um ambiente para desenvolvimento de softwares bastante particular. Estima-se que centenas de projetos de softwares estão sendo desenvolvidos, com propósitos específicos, desde programas que auxiliam o trabalho da equipe de recursos humanos até os que monitoram o desempenho dos diversos experimentos do colisor de partículas. Apesar do setor de TI do CERN ter estipulado algumas regras para a política de segurança, muitos projetos de software pequenos não as seguem a risca. Isso implica em vulnerabilidades de programas utilizados em ambientes de produção no CERN.

    As regras de segurança estipuladas não são tão rígidas para estimular o meio acadêmico no avança das pesquisas científicas. Da mesma maneira, analisadores estáticos para assegurar a qualidade no desenvolvimento de códigos livres não são impostos para a comunidade presente. Existem ferramentas disponíveis e com documentação e suporte da equipe de TI, mas nenhum projeto é obrigado a utilizá-las. No caso deste projeto de mestrado, a linguagem computacional utilizada é o Python, sendo o \emph{PyLint}~\cite{pylint} uma ferramenta de análise estática cuja utilização é incentivada pelo CERN~\cite{cern_recomendations_tools}.

    Recentemente, o setor de segurança em computação do CERN contratou um serviço (\emph{Coverity}~\cite{Bessey:2010:FBL:1646353.1646374}) para garantir qualidade no desenvolvimento de novas funcionalidades no seu principal produto de análises físicas (o ROOT~\cite{BRUN}). O ROOT é utilizado por cerca de 10.000 pesquisadores e a integridade do software passou a ser vista como algo valioso. Um \emph{bug} perpetuado no ROOT pode ter um impacto muito negativo nas análises de resultados dos dados gerados pelo LHC. Assim que incorporado, o \emph{Coverity} já foi capaz de identificar mais de 40.000 falhas em aproximadamente 50 milhões de linhas de códigos~\cite{coverity_numbers}. Um ponto negativo observado por desenvolvedores do ROOT é o tempo necessário para executar o analisador estático: são 28 horas para cada 2 milhões de linhas de código. É válido observar que a ferramenta escolhida para o ROOT não atende as necessidades deste projeto de mestrado: o \emph{Coverity} atende códigos escritos em linguagem C/C++ enquanto os códigos do Tile-in-ONE (seção~\ref{tio}) são implementados em Python. Além disso, deseja-se aplicar mineração de dados em códigos fonte em um ambiente web.

    Em física de altas energias não existem artigos que indiquem a aplicação de mineração de dados em códigos fonte.

    \section{Métodos \emph{ensemble} em Árvores de Decisão}
    \label{ensemble}
    Árvore de decisão (DT, do inglês, \emph{Decision Tree})~\cite{Rokach:2008:DMD:1796114} é um algorítimo de aprendizado de máquina supervisionado. O termo ``supervisionado'' indica que, durante a fase de treinamento, as classes (ou alvos) são conhecidas. Desta maneira, o classificador aprende com casos em que se sabe a resposta (\emph{executável}/\emph{não executável}).

    Basicamente, o treinamento de uma árvore de decisão divide e agrupa o conjunto de treino em dois ou mais grupos homogêneos, sucessivamente. Para as divisões, o algorítimo se encarrega de escolher qual é o atributo mais significativo, ou seja, qual atributo irá tornar a divisão o mais homogênea (ou pura) possível. Tal processo consiste no que se conhece como \emph{splitting}.

    A figura[XXX] ilustra um exemplo clássico de árvore de decisão. Ele representa uma árvore que decide se uma pessoa vai ou não jogar tênis. Os atributos utilizados são condições do tempo (ensolarado, nublado ou chuvoso), umidade (alta ou baixa) e vento (forte ou fraco).

    Ainda sobre a figura[XXX], o conhecimento de algumas nomenclaturas é necessário. Elas serão referenciadas mais adiante. São elas:
    \begin{itemize}
        \item{O nó raiz (representado pela cor azul) representa todo o conjunto de treino e é onde as divisões em grupos heterogêneos começa.}
        \item{Os nós que continuam dividindo os sub-conjuntos restantes (em cinza) são denominados nós de decisão.}
        \item{Quando a divisão cessa, tem-se folhas (em verde). As folhas indicam a decisão tomada pela árvore treinada.}
    \end{itemize}

    Uma das vantagens em se utilizar árvores de decisão para classificação é a sua fácil compreensão. Para pessoas sem conhecimentos em inteligência computacional, o treinamento de classificadores e os resultados obtidos são de fácil compreensão. Além disso, a representação gráfica é intuitiva e permite que hipóteses sejam rapidamente relacionadas.

    Entretanto, existe uma desvantagem que faz com que árvores de decisão não sejam consideradas classificadores fortes: o fato de dividir conjuntos em sub-conjuntos homogêneos faz com que o classificador gerado fique especialista no conjunto de treino (\emph{overfit}). Ou seja, a árvore gerada sempre dependerá do conjunto de treino utilizado. O método \emph{ensemble} contorna essa questão levantada.

    \emph{Ensemble} são conjuntos de classificadores que combinam seus resultados individuais (por votação ou média ponderada) para classificar novas amostras~\cite{Seni:2010:EMD:1841412}. Uma das áreas mais ativas na área de aprendizado de máquina supervisionado tem sido \emph{ensemble}~\cite{Dietterich:2000:EMM:648054.743935}. Geralmente, classificadores \emph{ensemble} apresentam maior acurácia que classificadores individuais. A razão estatística para isso é que a variância de um somatório é menor que a variância individual. Computacionalmente, o treinamento pode dar-se de maneira mais rápida, uma vez que, em alguns casos é possível paralelizar a fase de treinamento.

    Classificadores \emph{ensemble} podem ser gerados para qualquer algorítimo, inclusive árvores de decisão. Tais classificadores são descritos nas sub-seções a seguir.

    \subsection{Bagged Trees}
    \subsection{Random Forest}
    \subsection{Boosted Decision Trees}